# A formal notion of learnability
## Summary
Given a class H of predictors from X to {0,1}, So h $\in$ H is a function from X to {0,1}. 
Now we define a simple learning paradigm $ERM_h$ that when given a sample outputs an h - $ERM_h (S) \in$  argmin($L_S (h)$) for h $\in$  H.

Theorem
If H is a finite class, for every f in H and every prob distribution D over X,
$P[L_D,f(ERM_h(S)) > \epsilon] \leq |H|. (1- \epsilon)^m \leq |H|.e^{- \epsilon m}$

And,
![](ben-david17.png)

![](ben-david18.png)
![](ben-david19.png)
The probability of a point falling in the area where h errs is greater than epsilon. Therefore prob that it'll look good $P[L_S (h) = 0]$ is bounded by $(1- \epsilon)^m$ 

Step2:
![](ben-david20.png)
Take a union of all *H* and have a bound on the probability that 

---

We say that a class of predictors *H* is PAC learnable if there exists a function $m_h: (0,1), (0,1) \rightarrow N$ such that there exists a learner $A: \bigcup_{m = 1}^{\infty} (X \times  \left \{ 0,1 \right \})^m \rightarrow \left \{ f|f: X \rightarrow  \left \{ 0,1 \right \} \right \}$ such that for every probability distribution *D* over *X, every f* in *H* and every $\epsilon, \delta > 0$, $Pr_{S \sim  D^m, f} [L_{D,f}(A(s)) > \epsilon]<\delta$ for $m \geq m_h(\epsilon, \delta)$

Accuracy: $\epsilon$ (Approximately)
Confidence: $\delta$ (Probably)
- Learner: Take as input sample (or union of samples) and outputs a predictor (which is a function)
- You give to the learner $\epsilon$ and $\delta$, with which he determines $m_h$, and according to  $m \geq m_h(\epsilon, \delta)$, the learner will tell you what sample size will suffice.

> What happened to finding a close enough *h*? How did we solve the problem in the end?
> Are accuracy and confidence independent of each other?
> So finding a good *h* purely comes from how big the sample size is?
> What if there is no good *h* (sample is misleading)? Theorem and realisability

Every finite H is PAC learnable with $m_h \leq \frac{ln(H) + ln(1/\delta)}{\epsilon}$. Furthermore, any $ERM_h$ learner will be successful
		- Comes from $Pr_{S \sim  D^m, f} [L_{D,f}(A(s)) > \epsilon]  \leq \left | H \right | e^{-\epsilon m} <\delta$
Therefore, finite classes are PAC learnable.
		
Advantages of this def of PAC learnability:
1. Distribution free guarentee (m must be greater than mh, which is only dependent on epsilon and delta)

Disadvantages
1. It only works if the labelling rule f comes from H. (Realizibility)

> But maybe, *f* is not in H. Maybe it is not possible to classify a papaya just based on colour and softness. Maybe there are two papayas of identical taste and colour but one is tasty, the other is not. Maybe samples are not iid.
		> When is a sample misleading again?

Assumption that labelling rule comes from *H* is too strong.
Therefore relaxation:
- Data is generated by some prob dist D over XxY, so even the labelling is random. We don't have a given data set Y. 
> But again, not every 2D vector is a papaya, not every 1D point picked from the distribution can represent a *tasty*/*non-tasty*
-  (Not only do we no longer assume f, the labeling rule comes from H, but we also assume there is no labelling rule)

> 1. Is this unsupervised learning? Reinforcement?

The best thing you can do here -
***Bayes Rule***
$h^{\ast} =$
![](ben-david21.png)

- *Given X, what is the prob that X is labelled 1?* If it is more than half, predict 1. 
- But we can't compute the prob, we don't know D

> 2. Causation/correlation somewhere?
> Kronecker-delta/indicator function

Problem with Bayes rule-
We can't calculate prob because we don't know the distribution. We only see a sample.
--> But the Bayes optimal predictor gives you a benchmark

[Prove that Bayes is optimal](https://cs.stackexchange.com/questions/72295/showing-that-bayes-classifier-is-optimal)
[What exactly Bayes Error is](https://stats.stackexchange.com/questions/302900/what-is-bayes-error-in-machine-learning)

Therefore, redefine successful learning to only have a *relative error guarentee*. Rather than error lesser than epsilon.

## Agnostic PAC Learnability
A class of predictors *H* is A-PAC learable if there exists some function $m_h: (0,1), (0,1) \rightarrow N$ and there exists a learner $A: \bigcup_{m = 1}^{\infty} (X \times  \left \{ 0,1 \right \})^m \rightarrow \left \{ f|f: X \rightarrow  \left \{ 0,1 \right \} \right \}$ such that for every probability distribution *D* over *X, Y*  and every $\epsilon, \delta > 0$, $Pr_{S \sim  D^m} [L_{D}(A(s)) > min_{h \in H} (L_{D}(h)) + \epsilon]<\delta$ for $m \geq m_h(\epsilon, \delta)$

![](ben-david26.png)
![](ben-david27.png)

(In the realisability case, it was $L_{D,f}(h) > 0$)

Instead of guarenteeing we never make an error greater than epsilon, now we guarentee that we will never get epsilon worse than the best $h \in H$ (therefore, relative), never greater than epsilon more than necessary (which is error h gives)

- epsilon gives slack
- best h in H is just comparison metric now. If there is no best h, then bayes 50 percent is metric.

> 3. Difference between PAC and APAC
> 4. Give example of APAC
> 5. So instead of finding f (we never had to?), are we just comparing to h? Diff between h here anf f there?

Therefore, lot of conditions completely relaxed, no *f* no rectangles


> DIfference between different *H* classes? It's harder to compete in a bigger *H*. Smaller *H* is easier to learner. As the class gets larger, competing with more *h*s becomes harder. If a class has only one function, the learner will always output that function. 2 constant functions? Learner will see which constant is more likely, for all samples. 
> But all this doesn't matter, because learner is given the class he is dealing with.
> *Is that what inductive bias is?*
### New setup
![](ben-david22.png)
![](ben-david23.png)
*What is the probability of seeing x paired with label 1, given that I've seen x?*. If it is more than half, $h* = 1$
Bayes not accessible to learner because it involves knowing D. Only a sample is available to the learner.
- Note the difference now. There is no *f*, you're literally comparing *h(x)* directly with the random *y* that was picked for the *x*

> 6. Now there's no f, and the generator picks a random x and a random y together. So picks a random papaya (or random values of colour and softness), and randomly assigns whether it is tasty or not. How does that work? Obviously here, Bayes is our absolute best bet. Noted, we can't calculate Bayes because we don't know the distribution. But how does it make sense to trust random distribution to mimic our true data? Wait what is this?

Even using Bayes, $L_D(h^\ast)$ might be high in some cases -
> 7. If we had an extremely skewed distribution (all papayas in the world are actually tasty), using the Bayes rule would lead to a mistake in labelling 50 percent of the data, wouldn't it? So we could still incur a pretty high loss. How is that possible? 

***The lack of ability to predict labels just comes from the fact that what we're measuring is irrelevant***

> 8. Yeah exactly, loss might be incurred due to measuring the wrong bloody parameters
> 9. What has prompted us to require a weaker notion of success?

Therefore, we need a -
## Weaker notion of success
![](ben-david24.png)
Trying to be as good as anybody in *H*
![](ben-david25.png)

So not even Bayes wala *h* anymore. A more realistic one, *h'*
Previously, class *H* was the guarenteed class in which we'd find *f*
There is no *f* now, and *H* is only a *benchmark* class.

Smaller the epsilon, more the training data we'll need
### There are other tasks to perform

![](ben-david28.png)
![](ben-david29.png)

> 10. How do you define success for these?
> 10. Difference between regression and multiclass?
> Relative penalty for regression, multiclass you lose or you win

### Therefore, an even more general setup
[[Learning - General setup]]